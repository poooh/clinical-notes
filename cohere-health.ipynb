{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b23af2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0c05168",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/rakshit/Documents/learn/git/clinical-notes/data/training_20180910'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir = os.path.abspath(\"data/training_20180910\")\n",
    "data_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e12b4b",
   "metadata": {},
   "source": [
    "## Try to read annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5862c28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tpat = re.compile(\"(T\\d+)\\s+([a-zA-Z\\-]+)\\s+(\\d+\\s+\\d+(\\s*;\\s*\\d+\\s+\\d+)*)\\s+(.*)\")\n",
    "rpat = re.compile(\"(R\\d+)\\s+([a-zA-Z\\-]+)\\s+Arg1:(T\\d+)\\s+Arg2:(T\\d+)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d7a594",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/training_20180910/107047.ann\") as f:\n",
    "    for line in f:\n",
    "        if line.startswith(\"T\"):\n",
    "            match = re.match(tpat, line.strip())\n",
    "            groups = match.groups()\n",
    "            pos = groups[2].split(\";\")\n",
    "            se = pos[0].strip().split()\n",
    "            start = int(se[0].strip())\n",
    "            if len(pos) > 1:\n",
    "                se = pos[-1].strip().split()\n",
    "            end = int(se[1].strip())\n",
    "            positions = [{\"start\": int(se[0].strip()), \"end\": int(se[1].strip())}]\n",
    "#             for p in pos:\n",
    "#                 se = p.strip().split()\n",
    "#                 positions.append({\"start\": int(se[0].strip()), \"end\": int(se[1].strip())})\n",
    "            if groups[4] == \"\"\n",
    "            print({\n",
    "                \"tag\": groups[0],\n",
    "                \"entity\": groups[1],\n",
    "                \"positions\": positions,\n",
    "                \"text\": groups[4]\n",
    "            })\n",
    "            break\n",
    "        else:\n",
    "            match = re.match(rpat, line.strip())\n",
    "            groups = match.groups()\n",
    "            print({\n",
    "                \"tag\": groups[0],\n",
    "                \"entity\": groups[1],\n",
    "                \"arg1\": groups[2],\n",
    "                \"arg2\": groups[3]\n",
    "            })\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c028c1b",
   "metadata": {},
   "source": [
    "## Try to read patient file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386dcb82",
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_keys = {\n",
    "    re.compile(\"admission date:\\s*\\[\\*+(\\d+\\-\\d+\\-\\d+)\\*+\\]\"): \"Admission Date:\",\n",
    "    re.compile(\"discharge date:\\s*\\[\\*+(\\d+\\-\\d+\\-\\d+)\\*+\\]\"): \"Discharge Date:\",\n",
    "    re.compile(\"date of birth:\\s*\\[\\*+(\\d+\\-\\d+\\-\\d+)\\*+\\]\"): \"Date of Birth:\",\n",
    "    re.compile(\"sex:\\s*([mf])\"): \"Sex:\",\n",
    "    re.compile(\"service:\\s*(.*)\"): \"Service:\",\n",
    "    re.compile(\"attending:\\s*\\[\\*+(.*)\\*+\\]\"): \"Attending:\",\n",
    "    re.compile(\"present illness:\\s*(.*)\"): \"History of Present Illness:\",\n",
    "    re.compile(\"allergies:\\s*(.*)\"): \"Allergies:\",\n",
    "    re.compile(\"chief complaint:\\s*(.*)\"): \"Chief Complaint:\",\n",
    "    re.compile(\"major surgical or invasive procedure:\\s*(.*)\"): \"Major Surgical or Invasive Procedure:\",\n",
    "    re.compile(\"past medical history:\\s*(.*)\"): \"Past Medical History:\",\n",
    "    re.compile(\"social history:\\s*(.*)\"): \"Social History:\",\n",
    "    re.compile(\"family history:\\s*(.*)\"): \"Family History:\",\n",
    "    re.compile(\"physical exam:\\s*(.*)\"): \"Physical Exam:\",\n",
    "    re.compile(\"pertinent results:\\s*(.*)\"): \"Pertinent Results:\",\n",
    "    re.compile(\"clinical information:\\s*(.*)\"): \"CLINICAL INFORMATION:\",\n",
    "    re.compile(\"findings:\\s*(.*)\"): \"FINDINGS:\",\n",
    "    re.compile(\"brief hospital course:\\s*(.*)\"): \"Brief Hospital Course:\",\n",
    "    re.compile(\"medications on admission:\\s*(.*)\"): \"Medications on Admission:\",\n",
    "    re.compile(\"discharge medications:\\s*(.*)\"): \"Discharge Medications:\",\n",
    "    re.compile(\"discharge disposition:\\s*(.*)\"): \"Discharge Disposition:\",\n",
    "    re.compile(\"facility:\\s*(.*)\"): \"Facility:\",\n",
    "    re.compile(\"discharge diagnosis:\\s*(.*)\"): \"Discharge Diagnosis:\",\n",
    "    re.compile(\"discharge condition:\\s*(.*)\"): \"Discharge Condition:\",\n",
    "    re.compile(\"discharge instructions:\\s*(.*)\"): \"Discharge Instructions:\",\n",
    "    re.compile(\"followup instructions:\\s*(.*)\"): \"Followup Instructions:\"\n",
    "}\n",
    "multi_line_patient_keys = {\n",
    "    \"Allergies:\", \"Chief Complaint:\", \"Major Surgical or Invasive Procedure:\",\n",
    "    \"History of Present Illness:\", \"Past Medical History:\", \"Social History:\",\n",
    "    \"Family History:\", \"Physical Exam:\", \"Pertinent Results:\", \"CLINICAL INFORMATION:\",\n",
    "    \"FINDINGS:\", \"Brief Hospital Course:\", \"Medications on Admission:\",\n",
    "    \"Discharge Medications:\", \"Discharge Disposition:\", \"Facility:\", \"Discharge Diagnosis:\",\n",
    "    \"Discharge Condition:\", \"Discharge Instructions:\", \"Followup Instructions:\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73106a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_file = {}\n",
    "key = None\n",
    "with open(\"data/training_20180910/134445.txt\") as f:\n",
    "    for line in f:\n",
    "        sl = line.strip()\n",
    "        single_line_data = {}\n",
    "        for pat, val in patient_keys.items():\n",
    "            match = re.search(pat, sl.lower())\n",
    "            if match:\n",
    "                single_line_data[val] = match.groups()[0] + \" \"\n",
    "        if single_line_data:\n",
    "            patient_file.update(single_line_data)\n",
    "            multi = multi_line_patient_keys.intersection(set(single_line_data.keys()))\n",
    "            if multi:\n",
    "                key = multi.pop()\n",
    "                continue\n",
    "            else:\n",
    "                key = None\n",
    "        if key:\n",
    "            patient_file[key] += line\n",
    "print(json.dumps(patient_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c81f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {}\n",
    "annotator_comments = {}\n",
    "for f in os.listdir(data_dir):\n",
    "    fp = os.path.join(data_dir, f)\n",
    "    if os.path.isfile(fp):\n",
    "        key = f[:-4]\n",
    "        if key not in data:\n",
    "            data[key] = {\"patient_id\": key}\n",
    "        if fp.endswith(\".ann\"):\n",
    "            if key not in annotator_comments:\n",
    "                annotator_comments[key] = []\n",
    "            with open(fp) as fl:\n",
    "                anns = {\"T\": [], \"R\": []}\n",
    "                for line in fl:\n",
    "                    if line.startswith(\"#\"):\n",
    "                        annotator_comments[key].append(line)\n",
    "                    elif line.startswith(\"T\"):\n",
    "                        match = re.match(tpat, line.strip())\n",
    "                        groups = match.groups()\n",
    "                        pos = groups[2].split(\";\")\n",
    "                        positions = []\n",
    "                        for p in pos:\n",
    "                            se = p.strip().split()\n",
    "                            positions.append({\"start\": se[0], \"end\": se[1]})\n",
    "                        anns[\"T\"].append({\n",
    "                            \"tag_type\": \"T\",\n",
    "                            \"tag\": groups[0],\n",
    "                            \"entity\": groups[1],\n",
    "                            \"positions\": positions,\n",
    "                            \"text\": groups[4]\n",
    "                        })\n",
    "                    else:\n",
    "                        match = re.match(rpat, line.strip())\n",
    "                        if not match:\n",
    "                            print(f)\n",
    "                            print(line)\n",
    "                            print(\"*\" * 80)\n",
    "                        groups = match.groups()\n",
    "                        anns[\"R\"].append({\n",
    "                            \"tag_type\": \"R\",\n",
    "                            \"tag\": groups[0],\n",
    "                            \"entity\": groups[1],\n",
    "                            \"arg1\": groups[2],\n",
    "                            \"arg2\": groups[3]\n",
    "                        })\n",
    "                data[key][\"annotations\"] = anns\n",
    "                data[key][\"annotations_dump\"] = json.dumps(anns)\n",
    "        elif fp.endswith(\".txt\"):\n",
    "            file_key = None\n",
    "            patient_file = {}\n",
    "            with open(fp) as fl:\n",
    "                text = fl.read()\n",
    "            data[key][\"full_text\"] = text\n",
    "            match = re.search(re.compile(\"present illness:\"), text.lower())\n",
    "            if match:\n",
    "                data[key][\"present_history_start\"] = match.span()[1]\n",
    "#             try:\n",
    "#                 data[key][\"present_history_start\"] = text.index(\"History of Present Illness:\") + 28\n",
    "#             except ValueError:\n",
    "#                 print(\"*\" * 80)\n",
    "#                 print(key)\n",
    "#                 print(\"*\" * 80)\n",
    "            for line in text.split(\"\\n\"):\n",
    "                sl = line.strip()\n",
    "                single_line_data = {}\n",
    "                for pat, val in patient_keys.items():\n",
    "                    match = re.search(pat, sl.lower())\n",
    "                    if match:\n",
    "                        single_line_data[val] = match.groups()[0] + \" \"\n",
    "                if single_line_data:\n",
    "                    patient_file.update(single_line_data)\n",
    "                    multi = multi_line_patient_keys.intersection(set(single_line_data.keys()))\n",
    "                    if multi:\n",
    "                        file_key = multi.pop()\n",
    "                        continue\n",
    "                    else:\n",
    "                        file_key = None\n",
    "                if file_key:\n",
    "                    patient_file[file_key] += line\n",
    "            data[key].update(patient_file)\n",
    "        else:\n",
    "            raise IOError(f\"Error Reading file {fp}\")\n",
    "        if \"present_history_start\" in data[key] and \"annotations\" in data[key]:\n",
    "            data[key][\"present_history_end\"] = data[key][\"present_history_start\"] + len(data[key][\"History of Present Illness:\"])\n",
    "            atomic = pd.DataFrame(data[key][\"annotations\"][\"T\"])\n",
    "            relations = pd.DataFrame(data[key][\"annotations\"][\"R\"])\n",
    "            anf = atomic[atomic.positions.map(lambda x: any(int(xi[\"start\"]) < data[key][\"present_history_end\"] and int(xi[\"start\"]) >= data[key][\"present_history_start\"] for xi in x))]\n",
    "            rnf = relations[relations.arg1.isin(anf.tag)&relations.arg2.isin(anf.tag)]\n",
    "            hist_anns = {\"T\": anf.to_dict(orient=\"records\"), \"R\": rnf.to_dict(orient=\"records\")}\n",
    "            data[key][\"present_history_annotations\"] = hist_anns\n",
    "            data[key][\"present_history_annotations_dump\"] = json.dumps(hist_anns)\n",
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f375fcaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "[(key, val) for key, val in annotator_comments.items() if val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55e58c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(list(data.values()))\n",
    "df = df.drop([\"present_history_annotations\", \"annotations\"], axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0429573",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df[[\"patient_id\", \"Chief Complaint:\", \"History of Present Illness:\", \"present_history_annotations_dump\"]]\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1257a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"data/complete_patient_data.csv\", index=False)\n",
    "df2.to_csv(\"data/present_history_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec3e3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/training_20180910/134445.txt\") as f:\n",
    "    text = f.read()\n",
    "text[:2000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ba1b1b",
   "metadata": {},
   "source": [
    "## Try keyword extraction with scispacy models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd6329b",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_lg = spacy.load(\"en_core_sci_lg\")\n",
    "# nlp_sm = spacy.load(\"en_core_sci_sm\")\n",
    "# nlp_md = spacy.load(\"en_core_sci_md\")\n",
    "nlp = spacy.load(\"en_core_sci_scibert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b4d051",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = data[\"100035\"][\"History of Present Illness:\"]\n",
    "nlp(text).ents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e4cf8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = data[\"100035\"][\"History of Present Illness:\"]\n",
    "nlp_lg(text).ents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1115b93a",
   "metadata": {},
   "source": [
    "## Try keyword extraction on \"Chief Complaint:\" sections to see different kinds of complaints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3f7b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "complaint = data[\"106621\"][\"Chief Complaint:\"]\n",
    "doc2 = nlp(complaint)\n",
    "doc2.ents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c685e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "complaints = {}\n",
    "for pid, patient_file in data.items():\n",
    "    complaint = patient_file.get(\"Chief Complaint:\", \"\")\n",
    "    cdoc = nlp(complaint)\n",
    "    complaints[pid] = cdoc.ents\n",
    "complaints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02115419",
   "metadata": {},
   "outputs": [],
   "source": [
    "complaint_docs = {}\n",
    "for pid, patient_file in data.items():\n",
    "    complaint = patient_file.get(\"Chief Complaint:\", \"\")\n",
    "    cdoc = nlp(complaint.lower())\n",
    "    text = patient_file.get(\"History of Present Illness:\", \"\")\n",
    "    doc = nlp(text.lower())\n",
    "    for ent in cdoc.ents:\n",
    "        key = ent.text.strip()\n",
    "        if key not in complaint_docs:\n",
    "            complaint_docs[key] = []\n",
    "        complaint_docs[key].append(doc)\n",
    "complaint_docs.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90beb286",
   "metadata": {},
   "source": [
    "## Check similarity scores of keywords extracted from \"History of Present Illness:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c8b870",
   "metadata": {},
   "outputs": [],
   "source": [
    "set1 = set(ent.text for ent in complaint_docs[\"hypotension\"][0].ents)\n",
    "set1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7a8b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_tag = ['PROPN', 'ADJ', 'NOUN']\n",
    "set2 = set(token.text for token in complaint_docs[\"hypotension\"][0] if token.text not in nlp.Defaults.stop_words and token.pos_ in pos_tag)\n",
    "set2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a7a4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "set3 = set1.intersection(set2)\n",
    "set3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682ca69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_1=nlp_lg(\"hypotension\")\n",
    "for tok in set3:\n",
    "    token_2=nlp_lg(tok)\n",
    "    similarity_score=token_1.similarity(token_2)\n",
    "    print(tok, \"---\", similarity_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826e1f02",
   "metadata": {},
   "source": [
    "## Above scores show words like \"lightheadedness\", \"myalgias\", \"pain\", etc from \"History  of Present Illness\" section having higher scores with the word \"hypotension\" from \"Chief Complaint\". This could be good news."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8df62a6",
   "metadata": {},
   "source": [
    "## ========================================================================"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63fdea7e",
   "metadata": {},
   "source": [
    "## Try to get entities from \"History of Present Illness\" sorted based on the above similarity scores and limit to top 20 entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e65b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_complaint_entities = {}\n",
    "for complaint, docs in complaint_docs.items():\n",
    "    if not complaint:\n",
    "        continue\n",
    "    token_1=nlp_lg(complaint)\n",
    "    ents = []\n",
    "    doc_ents = list(set(ent.text for doc in docs for ent in doc.ents))\n",
    "    for ent in doc_ents:\n",
    "        token_2=nlp_lg(ent)\n",
    "        similarity_score=token_1.similarity(token_2)\n",
    "        ents.append((-similarity_score, ent))\n",
    "    top_complaint_entities[complaint] = list(zip(*(sorted(ents)[:20])))[1]\n",
    "top_complaint_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aed350d",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_1=nlp_lg(\"sob\")\n",
    "token_2=nlp_lg(\"vancomycin 1 gm\")\n",
    "token_1.similarity(token_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d39dfef8",
   "metadata": {},
   "source": [
    "## Next, we extract top keywords on document level. We do it in following steps:\n",
    "### - Extract entities from \"Chief Complaint:\" section.\n",
    "### - Extract top 10 keywords for each entity from \"History of Present Illness:\" section\n",
    "### - Sort all top keywords based on combined similarity score and keep top 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3541511",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keywords(row):\n",
    "    complaint = row[\"Chief Complaint:\"].lower()\n",
    "    text = row[\"History of Present Illness:\"].lower()\n",
    "    # If we were not able to extract complaint or history, skip\n",
    "    if not (complaint and text):\n",
    "        row[\"top_keywords\"] = []\n",
    "        return row\n",
    "    \n",
    "    cdoc = nlp_lg(complaint)\n",
    "    doc = nlp_lg(text)\n",
    "    ents = {}\n",
    "    combined = {}\n",
    "    # Calculate scores for each entity in chief complaint with each entity\n",
    "    # in history of present illness.\n",
    "    for cent in cdoc.ents:\n",
    "        token_1=nlp_lg(cent.text)\n",
    "        ents[cent.text] = []\n",
    "        for ent in doc.ents:\n",
    "            token_2=nlp_lg(ent.text)\n",
    "            similarity_score=token_1.similarity(token_2)\n",
    "            if ent.text not in combined:\n",
    "                combined[ent.text] = 1\n",
    "            # We will take combined score as the multiplication of all the scores for now\n",
    "            combined[ent.text] *= similarity_score\n",
    "            ents[cent.text].append({\"score\": similarity_score, \"entity\": ent.text})\n",
    "    # Add combined score with all complaint entities to each history entity\n",
    "    result_ents = {}\n",
    "    for cent, history_ents in ents.items():\n",
    "        result_ents[cent] = []\n",
    "        for ent in history_ents:\n",
    "            ent[\"combined_score\"] = combined[ent[\"entity\"]]\n",
    "            result_ents[cent].append(ent)\n",
    "    # Sort and get top 10 history entities for each complaint entity\n",
    "    ents = []\n",
    "    for cent in result_ents:\n",
    "        top10 = sorted(result_ents[cent], key=lambda x: -x[\"score\"])[:10]\n",
    "        ents += top10\n",
    "    # Finally sort and take top 10 entities based on combined score\n",
    "    row[\"top_keywords\"] = json.dumps([ent[\"entity\"] for ent in sorted(ents, key=lambda x: -x[\"combined_score\"])[:10]])\n",
    "    return row\n",
    "\n",
    "df3 = df2.fillna(\"\").apply(extract_keywords, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c237a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05df520",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.to_csv(\"data/output.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56975793",
   "metadata": {},
   "source": [
    "## The above method captures some important keywords.\n",
    "## The problem with the method is - because of similarity score, the method is only able to pickup words pertaining to the disease/problem and not the medicines if any were given.\n",
    "## This could be because spacy's model is a statistical model and the model might have not seen the medicines in context of the issues.\n",
    "\n",
    "## So we will try to train the entity model to give better entities.\n",
    "## We find the clinical notes data to train our model from Harvard DBMI portal - https://portal.dbmi.hms.harvard.edu/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1897d059",
   "metadata": {},
   "source": [
    "## ======================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87936065",
   "metadata": {},
   "source": [
    "## We will do the task in 3 steps:\n",
    "### - First we preprocess the data to convert it into spacy binary format.\n",
    "### - Train the NER model using spacy train\n",
    "### - Evaluate the NER model using spacy evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0587bbbd",
   "metadata": {},
   "source": [
    "# Preprocessing:\n",
    "\n",
    "## NOTE: My request to data access is pending, so, for now, let's assume that the data is in same format as this assignment. If there are any changes, corresponding changes can be made to the preprocess script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "20d7cf45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import shutil\n",
    "\n",
    "from typing import Any, Dict, List\n",
    "\n",
    "from spacy.util import get_words_and_spaces, compile_infix_regex\n",
    "from spacy.tokens import Doc, DocBin\n",
    "from spacy.tokenizer import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "afe7a337",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dir = os.path.abspath(os.path.join(\"data\", \"data\"))\n",
    "corpus_dir = os.path.abspath(os.path.join(\"data\", \"corpus\"))\n",
    "if not os.path.isdir(corpus_dir):\n",
    "    os.makedirs(corpus_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "a9d82287",
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_train_eval(old_dir: str, new_dir: str, split_ratio: float=0.8):\n",
    "    \"\"\"\n",
    "    Function to separate data in training and validation.\n",
    "    \n",
    "    Args:\n",
    "        old_dir: str, Current data directory\n",
    "        new_dir: str, New data directory with train and validation separated\n",
    "        split_ratio: float, Fraction of data to keep in training.\n",
    "                     Defaults to 80-20 train and validation split.\n",
    "    \n",
    "    Returns: None\n",
    "    \"\"\"\n",
    "    train_dir = os.path.join(new_dir, \"train\")\n",
    "    eval_dir = os.path.join(new_dir, \"eval\")\n",
    "    if not os.path.isdir(train_dir):\n",
    "        os.makedirs(train_dir)\n",
    "    if not os.path.isdir(eval_dir):\n",
    "        os.makedirs(eval_dir)\n",
    "    for f in os.listdir(old_dir):\n",
    "        ofp = os.path.join(old_dir, f)\n",
    "        if os.path.isfile(ofp) and ofp.endswith(\".txt\"):\n",
    "            if random.random() <= split_ratio:\n",
    "                nfp = os.path.join(train_dir, f)\n",
    "            else:\n",
    "                nfp = os.path.join(eval_dir, f)\n",
    "            shutil.copy(ofp, nfp)\n",
    "            shutil.copy(ofp.replace(\".txt\", \".ann\"), nfp.replace(\".txt\", \".ann\"))\n",
    "            \n",
    "def get_file_paths(data_dir: str) -> Dict[str, Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Function to return all file paths from a data directory in a json format.\n",
    "    \n",
    "    Args:\n",
    "        data_dir: str, Data directory containing annotation and patient files.\n",
    "    \n",
    "    Return: dict\n",
    "    \"\"\"\n",
    "    files = {}\n",
    "    for f in os.listdir(data_dir):\n",
    "        fp = os.path.join(data_dir, f)\n",
    "        if os.path.isfile(fp):\n",
    "            patient_id = f[:-4]\n",
    "            if patient_id not in files:\n",
    "                files[patient_id] = {}\n",
    "            if f.endswith(\".ann\"):\n",
    "                files[patient_id][\"annotation\"] = fp\n",
    "            elif f.endswith(\".txt\"):\n",
    "                files[patient_id][\"data\"] = fp\n",
    "            else:\n",
    "                raise IOError(f\"File type for {fp} not supported.\")\n",
    "#     print(files)\n",
    "    return files\n",
    "\n",
    "def read_annotations(fp: str) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Function to read annotations from a given annotation file.\n",
    "    Args:\n",
    "        fp: str, File path of an annotation file\n",
    "    \n",
    "    Returns: dict\n",
    "        Annotations in a dictionary format.\n",
    "        \n",
    "    Sample: [\n",
    "        {\"start\":10, \"end\":15, \"label\":\"DRUG\"}\n",
    "        ...\n",
    "        {\"start\":200, \"end\":208, \"label\":\"REASON\"}\n",
    "    ]\n",
    "    \"\"\"\n",
    "    annotations = []\n",
    "    with open(fp) as f:\n",
    "        for line in f:\n",
    "            if line.startswith(\"T\"):\n",
    "                match = re.match(tpat, line.strip())\n",
    "                groups = match.groups()\n",
    "                pos = groups[2].split(\";\")\n",
    "                se = pos[0].strip().split()\n",
    "                start = int(se[0].strip())\n",
    "                if len(pos) > 1:\n",
    "                    se = pos[-1].strip().split()\n",
    "                end = int(se[1].strip())\n",
    "                ann = {\n",
    "                    \"start\": start,\n",
    "                    \"end\": end,\n",
    "                    \"label\": groups[1],\n",
    "                    \"text\": groups[4]\n",
    "                }\n",
    "                annotations.append(ann)\n",
    "#                 positions = []\n",
    "#                 for p in pos:\n",
    "#                     se = p.strip().split()\n",
    "#                     positions.append({\"start\": int(se[0].strip()), \"end\": int(se[1].strip())})\n",
    "#                 for pos in positions:\n",
    "#                     ann = { \"label\": groups[1], \"text\": groups[4] }\n",
    "#                     ann.update(pos)\n",
    "#                     if not (ann[\"start\"] and ann[\"end\"] and ann[\"label\"].strip()):\n",
    "#                         print(ann)\n",
    "#                         print(fp)\n",
    "#                         print(\"=\" * 80)\n",
    "#                     annotations.append(ann)\n",
    "    return annotations\n",
    "\n",
    "def preprocess(ip_path: str, op_path: str):\n",
    "    \"\"\"\n",
    "    Function to convert the given data in spacy binary format.\n",
    "    \n",
    "    Args:\n",
    "        ip_path: str, Directory containing both the patient files and annotation files.\n",
    "        op_path: str, Path where to save the spacy binary data.\n",
    "    \n",
    "    Returns: None\n",
    "    \"\"\"\n",
    "    files = get_file_paths(ip_path)\n",
    "    doc_bin = DocBin(attrs=[\"ENT_IOB\", \"ENT_TYPE\"])\n",
    "    w = 0\n",
    "    for patient_id, val in files.items():\n",
    "        if \"annotation\" not in val:\n",
    "            print(\"Annotation file not given for\", patient_id)\n",
    "        annotations = read_annotations(val[\"annotation\"])\n",
    "        if \"data\" not in val:\n",
    "            print(\"Data file not given for\", patient_id)\n",
    "        with open(val[\"data\"]) as f:\n",
    "            text = f.read()\n",
    "        doc = nlp.make_doc(text)\n",
    "        toks = [token.text for token in doc]\n",
    "        dents = []\n",
    "        nanns = []\n",
    "        for ann in annotations:\n",
    "            if text[ann[\"start\"]:ann[\"end\"]] in toks:\n",
    "                dents.append(doc.char_span(ann[\"start\"], ann[\"end\"], label=ann[\"label\"]))\n",
    "                nanns.append(ann)\n",
    "        doc.ents = dents\n",
    "#         try:\n",
    "#             doc.ents = dents\n",
    "# #             doc.ents = [\n",
    "# #                 doc.char_span(ann[\"start\"], ann[\"end\"], label=ann[\"label\"])\n",
    "# #                 for ann in annotations\n",
    "# #             ]\n",
    "#         except TypeError:\n",
    "#             for ann in nanns:\n",
    "# #                 toks = [token.text for token in doc]\n",
    "#                 if text[ann[\"start\"]:ann[\"end\"]] not in toks:\n",
    "#                     print(patient_id, ann)\n",
    "#                     break\n",
    "# #             print(toks)\n",
    "#             w += 1\n",
    "# #             print(annotations)\n",
    "#             return\n",
    "        doc_bin.add(doc)\n",
    "    print(w, len(files))\n",
    "    doc_bin.to_disk(op_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c6f37928",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyTokenizer(Tokenizer):\n",
    "    def __call__(self, text):\n",
    "        pat = r\"(\\d+)\\s*(m?g)\"\n",
    "        text = re.sub(pat, lambda m: \"{}_{}\".format(m.group(1), m.group(2)), text)\n",
    "        doc = super().__call__(text)\n",
    "        pat = r\"(\\d+)_(m?g)\"\n",
    "        new_toks = [re.sub(pat, lambda m: \"{} {}\".format(m.group(1), m.group(2)), tok.text) for tok in doc]\n",
    "\n",
    "        return Doc(doc.vocab, words=new_toks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "5d4fa076",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.blank(\"en\")\n",
    "nlp.tokenizer = MyTokenizer(\n",
    "        nlp.vocab,\n",
    "        prefix_search=nlp.tokenizer.prefix_search,\n",
    "        suffix_search=nlp.tokenizer.suffix_search,\n",
    "        infix_finditer=nlp.tokenizer.infix_finditer,\n",
    "        token_match=nlp.tokenizer.token_match,\n",
    ")\n",
    "# suffixes = nlp.Defaults.suffixes + [r\"\\d+\\s*m?g\"]\n",
    "# suffix_regex = spacy.util.compile_suffix_regex(suffixes)\n",
    "# nlp.tokenizer.suffix_search = suffix_regex.search\n",
    "# infixes = ([r\"\\d+\\s*m?g\"])\n",
    "# infix_re = compile_infix_regex(infixes)\n",
    "# nlp.tokenizer.infix_finditer = infix_re.finditer\n",
    "# tokenizer = spacy.load(\"en_core_sci_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "dafe0b18",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'NoneType' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [77]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m train_dir \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(new_dir, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m eval_dir \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(new_dir, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meval\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m \u001b[43mpreprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcorpus_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain.spacy\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [72]\u001b[0m, in \u001b[0;36mpreprocess\u001b[0;34m(ip_path, op_path)\u001b[0m\n\u001b[1;32m    129\u001b[0m                 dents\u001b[38;5;241m.\u001b[39mappend(doc\u001b[38;5;241m.\u001b[39mchar_span(ann[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstart\u001b[39m\u001b[38;5;124m\"\u001b[39m], ann[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mend\u001b[39m\u001b[38;5;124m\"\u001b[39m], label\u001b[38;5;241m=\u001b[39mann[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m]))\n\u001b[1;32m    130\u001b[0m                 nanns\u001b[38;5;241m.\u001b[39mappend(ann)\n\u001b[0;32m--> 131\u001b[0m         doc\u001b[38;5;241m.\u001b[39ments \u001b[38;5;241m=\u001b[39m dents\n\u001b[1;32m    132\u001b[0m \u001b[38;5;66;03m#         try:\u001b[39;00m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;66;03m#             doc.ents = dents\u001b[39;00m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;66;03m# #             doc.ents = [\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;66;03m# #             print(annotations)\u001b[39;00m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;66;03m#             return\u001b[39;00m\n\u001b[1;32m    148\u001b[0m         doc_bin\u001b[38;5;241m.\u001b[39madd(doc)\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.8/site-packages/spacy/tokens/doc.pyx:745\u001b[0m, in \u001b[0;36mspacy.tokens.doc.Doc.ents.__set__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.8/site-packages/spacy/tokens/doc.pyx:1755\u001b[0m, in \u001b[0;36mspacy.tokens.doc.get_entity_info\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
     ]
    }
   ],
   "source": [
    "# separate_train_eval(data_dir, new_dir)\n",
    "train_dir = os.path.join(new_dir, \"train\")\n",
    "eval_dir = os.path.join(new_dir, \"eval\")\n",
    "preprocess(train_dir, os.path.join(corpus_dir, \"train.spacy\"))\n",
    "# preprocess(eval_dir, os.path.join(corpus_dir, \"eval.spacy\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cff29112",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'citalopram'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"data/training_20180910/107047.txt\") as f:\n",
    "    text = f.read()\n",
    "text[14083:14093]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "84e8405a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['20 mg']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[t.text for t in nlp(\"20 mg\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6608fdba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'20  dfdf'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = \"20  dfdf\"\n",
    "pat = r\"(\\d+)\\s*(m?g)\"\n",
    "re.sub(pat, lambda m: \"{}_{}\".format(m.group(1), m.group(2)), s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4213d50",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
